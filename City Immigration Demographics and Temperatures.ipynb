{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# City Immigration, Demographics and Temperatures\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project aims to be able to answers questions on US immigration such as what are the most popular cities for immigration, what is the gender distribution of the immigrants, what is the visa type distribution of the immigrants, what is the average age per immigrant and what is the average temperature per month per city. We extract data from 3 different sources, the I94 immigration dataset of 2016, city temperature data from Kaggle and US city demographic data from OpenSoft. We design 4 dimension tables: Cities, immigrants, monthly average city temperature and time, and 1 fact table: Immigration. We use Spark for ETL jobs and store the results in parquet for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from pprint import pprint\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import count, col, udf, year, month, avg, round, dayofweek, weekofyear, isnull\n",
    "from pyspark.sql.types import StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The goal of this project is pull data from 3 different sources and create fact and dimension table to be able to do analysis on US immigration using factors of city monthly average temperature, city demographics and seasonality.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "1. **I94 Immigration Data**: comes from the U.S. National Tourism and Trade Office and contains various statistics on international visitor arrival in USA and comes from the US National Tourism and Trade Office. The dataset contains data from 2016. [link](https://travel.trade.gov/research/reports/i94/historical/2016.html)\n",
    "2. **World Temperature Data**: comes from Kaggle and contains average weather temperatures by city. [link](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "3. **U.S. City Demographic Data**: comes from OpenSoft and contains information about the demographics of all US cities such as average age, male and female population. [link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read immigration data\n",
    "# Assumption for the project: in this project only the i94_apr16_sub.sas7bdat will be used, in order to all process all of the available files, simple use i94_files\n",
    "i94_files = glob.glob(\"../../data/18-83510-I94-Data-2016/*.sas7bdat\")\n",
    "i94_fname = \"../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\"\n",
    "i94_df = spark.read.format(\"com.github.saurfang.sas.spark\").load(i94_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read temperature data\n",
    "temperature_fname = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "temperature_df = spark.read.format(\"csv\").option(\"delimiter\", \",\").option(\"header\", \"true\").load(temperature_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read demographics data\n",
    "demo_fname = \"us-cities-demographics.csv\"\n",
    "demo_df = spark.read.format(\"csv\").option(\"delimiter\", \";\").option(\"header\", \"true\").load(demo_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "##### Data Exploration & Cleaning\n",
    "\n",
    "#### Immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Explore immigration data\n",
    "i94_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create list of valid ports\n",
    "i94_sas_label_descriptions_fname = \"I94_SAS_Labels_Descriptions.SAS\"\n",
    "with open(i94_sas_label_descriptions_fname) as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "re_compiled = re.compile(r\"\\'(.*)\\'.*\\'(.*)\\'\")\n",
    "valid_ports = {}\n",
    "for line in lines[302:961]:\n",
    "    results = re_compiled.search(line)\n",
    "    valid_ports[results.group(1)] = results.group(2)\n",
    "print(len(valid_ports))\n",
    "#pprint(valid_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create list of valid states\n",
    "valid_states = demo_df.toPandas()[\"State Code\"].unique()\n",
    "print(len(valid_states))\n",
    "print(valid_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create udf to convert SAS date to PySpark date \n",
    "@udf(StringType())\n",
    "def convert_datetime(x):\n",
    "    if x:\n",
    "        return (datetime(1960, 1, 1).date() + timedelta(x)).isoformat()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create udf to validate state\n",
    "@udf(StringType())\n",
    "def validate_state(x):  \n",
    "    if x in valid_states:\n",
    "        return x\n",
    "    return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean immigration data\n",
    "\n",
    "# Remove any missing values\n",
    "cleaned_i94_df = i94_df.dropna(how=\"any\", subset=[\"i94port\", \"i94addr\", \"gender\"])\n",
    "\n",
    "# Extract valid states \n",
    "cleaned_i94_df = cleaned_i94_df.withColumn(\"i94addr\", validate_state(cleaned_i94_df.i94addr))\n",
    "\n",
    "# Convert arrival_date (SAS format) to PySpark format\n",
    "cleaned_i94_df = cleaned_i94_df.withColumn(\"arrdate\", convert_datetime(cleaned_i94_df.arrdate))\n",
    "\n",
    "# only keep us related immigration data\n",
    "cleaned_i94_df = cleaned_i94_df.filter(cleaned_i94_df.i94addr != 'other')\n",
    "\n",
    "staging_i94_df = cleaned_i94_df.select(col(\"cicid\").alias(\"id\"), \n",
    "                                       col(\"arrdate\").alias(\"date\"),\n",
    "                                       col(\"i94port\").alias(\"city_code\"),\n",
    "                                       col(\"i94addr\").alias(\"state_code\"),\n",
    "                                       col(\"i94bir\").alias(\"age\"),\n",
    "                                       col(\"gender\").alias(\"gender\"),\n",
    "                                       col(\"i94visa\").alias(\"visa_type\"),\n",
    "                                       \"count\").drop_duplicates()\n",
    "\n",
    "staging_i94_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_i94_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temperature_df.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create udf to map city full name to city port abbreviation\n",
    "\n",
    "@udf(StringType())\n",
    "def city_to_port(city):\n",
    "    for key in valid_ports:\n",
    "        if city.lower() in valid_ports[key].lower():\n",
    "            return key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean temperature data\n",
    "\n",
    "# Only use temperatures from United States\n",
    "# Map full name to city port abbreviation\n",
    "# Remove invalid ports\n",
    "cleaned_temp_df = temperature_df.filter(temperature_df[\"Country\"] == \"United States\") \\\n",
    "    .withColumn(\"year\", year(temperature_df['dt'])) \\\n",
    "    .withColumn(\"month\", month(temperature_df[\"dt\"])) \\\n",
    "    .withColumn(\"i94port\", city_to_port(temperature_df[\"City\"])) \\\n",
    "    .withColumn(\"AverageTemperature\", col(\"AverageTemperature\").cast(\"float\")) \\\n",
    "    .dropna(how='any', subset=[\"i94port\"])\n",
    "\n",
    "# Only use temperatures from 2013 (the latest year in the dataset)\n",
    "cleaned_temp_df = cleaned_temp_df.filter(cleaned_temp_df[\"year\"] == 2013)\n",
    "\n",
    "staging_temp_df = cleaned_temp_df.select(col(\"year\"), col(\"month\"), col(\"i94port\").alias(\"city_code\"),\n",
    "                                         round(col(\"AverageTemperature\"), 1).alias(\"avg_temperature\"),\n",
    "                                         col(\"Latitude\").alias(\"lat\"), col(\"Longitude\").alias(\"long\")).drop_duplicates()\n",
    "\n",
    "print(staging_temp_df.count())\n",
    "staging_temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_temp_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Demographics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demo_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Clean demographics data\n",
    "\n",
    "# Calculate percentages of numeric columns and create new ones\n",
    "cleaned_demo_df = demo_df.withColumn(\"median_age\", demo_df['Median Age']) \\\n",
    "    .withColumn(\"pct_male_pop\", (demo_df['Male Population'] / demo_df['Total Population']) * 100) \\\n",
    "    .withColumn(\"pct_female_pop\", (demo_df['Female Population'] / demo_df['Total Population']) * 100) \\\n",
    "    .withColumn(\"pct_veterans\", (demo_df['Number of Veterans'] / demo_df['Total Population']) * 100) \\\n",
    "    .withColumn(\"pct_foreign_born\", (demo_df['Foreign-born'] / demo_df['Total Population']) * 100) \\\n",
    "    .withColumn(\"pct_race\", (demo_df['Count'] / demo_df['Total Population']) * 100) \\\n",
    "    .withColumn(\"city_code\", city_to_port(demo_df[\"City\"])) \\\n",
    "    .dropna(how='any', subset=[\"city_code\"])\n",
    "\n",
    "cleaned_demo_df = cleaned_demo_df.select(col(\"City\").alias(\"city_name\"), col(\"State Code\").alias(\"state_code\"), \n",
    "                         \"median_age\", \"pct_male_pop\", \"pct_female_pop\",\"pct_veterans\", \n",
    "                         \"pct_foreign_born\", col(\"Total Population\").alias(\"total_pop\"), \n",
    "                         col(\"Race\").alias(\"race\"), \"pct_race\").drop_duplicates()\n",
    "\n",
    "cleaned_demo_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Pivot the race column\n",
    "pivot_demo_df = cleaned_demo_df.groupBy(\"city_name\", \"state_code\", \"median_age\", \"pct_male_pop\",\n",
    "                                        \"pct_female_pop\",\"pct_veterans\", \"pct_foreign_born\", \"total_pop\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "pivot_demo_df = pivot_demo_df.withColumn(\"city_code\", city_to_port(pivot_demo_df[\"city_name\"])) \\\n",
    "    .dropna(how='any', subset=[\"city_code\"])\n",
    "\n",
    "staging_demo_df = pivot_demo_df.select(\"city_code\", \"state_code\", \"city_name\", \"median_age\",\n",
    "                                    round(col(\"pct_male_pop\"), 1).alias(\"pct_male_pop\"),\n",
    "                                    round(col(\"pct_female_pop\"), 1).alias(\"pct_female_pop\"),\n",
    "                                    round(col(\"pct_veterans\"), 1).alias(\"pct_veterans\"),\n",
    "                                    round(col(\"pct_veterans\"), 1).alias(\"pct_foreign_born\"),\n",
    "                                    round(col(\"American Indian and Alaska Native\"), 1).alias(\"pct_native_american\"),\n",
    "                                    round(col(\"Asian\"), 1).alias(\"pct_asian\"),\n",
    "                                    round(col(\"Black or African-American\"), 1).alias(\"pct_black\"),\n",
    "                                    round(col(\"Hispanic or Latino\"), 1).alias(\"pct_hispanic_or_latino\"),\n",
    "                                    round(col(\"White\"), 1).alias(\"pct_white\"), \"total_pop\")\n",
    "print(staging_demo_df.count())\n",
    "staging_demo_df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "staging_demo_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The star schema is chosen as the data model because it is simple and yet effective. users can write simple queries by joing fact and dimension tables to analyze the data.\n",
    "\n",
    "Here are the tables of the schema:\n",
    "\n",
    "##### Staging Tables\n",
    "```\n",
    "staging_i94_df\n",
    "    id\n",
    "    date\n",
    "    city_code\n",
    "    state_code\n",
    "    age\n",
    "    gender\n",
    "    visa_type\n",
    "    count\n",
    "    \n",
    "staging_temp_df\n",
    "    year\n",
    "    month\n",
    "    city_code\n",
    "    city_name\n",
    "    avg_temperature\n",
    "    lat\n",
    "    long\n",
    "    \n",
    "staging_demo_df\n",
    "    city_code\n",
    "    state_code\n",
    "    city_name\n",
    "    median_age\n",
    "    pct_male_pop\n",
    "    pct_female_pop\n",
    "    pct_veterans\n",
    "    pct_foreign_born\n",
    "    pct_native_american\n",
    "    pct_asian\n",
    "    pct_black\n",
    "    pct_hispanic_or_latino\n",
    "    pct_white\n",
    "    total_pop\n",
    "```\n",
    "\n",
    "##### Dimension Tables\n",
    "```\n",
    "immigrant_df\n",
    "    id\n",
    "    gender\n",
    "    age\n",
    "    visa_type\n",
    "    \n",
    "city_df\n",
    "    city_code\n",
    "    state_code\n",
    "    city_name\n",
    "    median_age\n",
    "    pct_male_pop\n",
    "    pct_female_pop\n",
    "    pct_veterans\n",
    "    pct_foreign_born\n",
    "    pct_native_american\n",
    "    pct_asian\n",
    "    pct_black\n",
    "    pct_hispanic_or_latino\n",
    "    pct_white\n",
    "    total_pop\n",
    "    lat\n",
    "    long\n",
    "    \n",
    "monthly_city_temp_df\n",
    "    city_code\n",
    "    year\n",
    "    month\n",
    "    avg_temperature\n",
    "    \n",
    "time_df\n",
    "    date\n",
    "    dayofweek\n",
    "    weekofyear\n",
    "    month\n",
    "```\n",
    "\n",
    "##### Fact Table\n",
    "```\n",
    "immigration_df\n",
    "    id\n",
    "    state_code\n",
    "    city_code\n",
    "    date\n",
    "    count\n",
    "```\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Steps necessary to pipeline the data into the chosen data model\n",
    "\n",
    "1. Clean the data on nulls, data types, duplicates, etc\n",
    "2. Load staging tables for staging_i94_df, staging_temp_df and staging_demo_df\n",
    "3. Create dimension tables for immigrant_df, city_df, monthly_city_temp_df and time_df\n",
    "4. Create fact table immigration_df with information on immigration count, mapping id in immigrant_df, city_code in city_df and monthly_city_temp_df and date in time_df ensuring referential integrity\n",
    "5. Save processed dimension and fact tables in parquet for downstream query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimension table for immigrant\n",
    "\n",
    "immigrant_df = staging_i94_df.select(\"id\", \"gender\", \"age\", \"visa_type\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigrant_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigrant_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimension table for city\n",
    "\n",
    "city_df = staging_demo_df.join(staging_temp_df, \"city_code\") \\\n",
    "    .select(\"city_code\", \"state_code\", \"city_name\", \"median_age\", \"pct_male_pop\", \"pct_female_pop\", \"pct_veterans\",\n",
    "           \"pct_foreign_born\", \"pct_native_american\", \"pct_asian\", \"pct_black\",\n",
    "           \"pct_hispanic_or_latino\", \"pct_white\", \"total_pop\", \"lat\", \"long\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "city_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimension table for monthly city temperature\n",
    "\n",
    "monthly_city_temp_df = staging_temp_df.select(\"city_code\", \"year\", \"month\", \"avg_temperature\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "monthly_city_temp_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "monthly_city_temp_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create dimension table for time\n",
    "\n",
    "time_df = staging_i94_df.withColumn(\"dayofweek\", dayofweek(\"date\"))\\\n",
    "                .withColumn(\"weekofyear\", weekofyear(\"date\"))\\\n",
    "                .withColumn(\"month\", month(\"date\"))\n",
    "                        \n",
    "time_df = time_df.select(\"date\", \"dayofweek\", \"weekofyear\", \"month\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "time_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create fact table for immigration\n",
    "\n",
    "immigration_df = staging_i94_df.select(\"id\", \"state_code\", \"city_code\", \"date\", \"count\").drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write dimension tables to parquet\n",
    "immigrant_df.write.mode(\"overwrite\").partitionBy(\"gender\", \"age\").parquet(\"immigrants\")\n",
    "city_df.write.mode(\"overwrite\").partitionBy(\"state_code\").parquet(\"cities\")\n",
    "monthly_city_temp_df.write.mode(\"overwrite\").parquet(\"monthly_city_temperatues\")\n",
    "time_df.write.mode(\"overwrite\").parquet(\"time\")\n",
    "\n",
    "# Write fact table to parquet\n",
    "immigration_df.write.mode(\"overwrite\").partitionBy(\"state_code\", \"city_code\").parquet(\"immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "def table_exists(df):\n",
    "    if df is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "if table_exists(immigrant_df) & table_exists(city_df) & table_exists(monthly_city_temp_df) & table_exists(time_df) & table_exists(immigration_df):\n",
    "    print(\"data quality check passed\")\n",
    "    print(\"dimension tables and fact table exist\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"data quality check failed\")\n",
    "    print(\"table missing...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def table_not_empty(df):\n",
    "    return df.count() != 0 \n",
    "\n",
    "if table_not_empty(immigrant_df) & table_not_empty(city_df) & table_not_empty(monthly_city_temp_df) & table_not_empty(time_df) & table_not_empty(immigration_df):\n",
    "    print(\"data quality check passed!\")\n",
    "    print(\"dimension tables and fact table contain records\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"data quality check failed!\")\n",
    "    print(\"null records...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Brief description of what the data is and where it came from.\n",
    "\n",
    "##### Dimension Tables\n",
    "```\n",
    "immigrant_df\n",
    "    id: id of immigrant\n",
    "    gender: gender of immigrant\n",
    "    age: age of immigrant\n",
    "    visa_type: immigrant's visa type\n",
    "    \n",
    "city_df\n",
    "    city_code: city port code\n",
    "    state_code: state code of the city\n",
    "    city_name: name of the city\n",
    "    median_age: median age of the city\n",
    "    pct_male_pop: city's male population in percentage\n",
    "    pct_female_pop: city's female population in percentage\n",
    "    pct_veterans: city's veteran population in percentage\n",
    "    pct_foreign_born: city's foreign born population in percentage\n",
    "    pct_native_american: city's native american population in percentage\n",
    "    pct_asian: city's asian population in percentage\n",
    "    pct_black: city's black population in percentage\n",
    "    pct_hispanic_or_latino: city's hispanic or latino population in percentage\n",
    "    pct_white: city's white population in percentage\n",
    "    total_pop: city's total population\n",
    "    lat: latitude of the city\n",
    "    long: longitude of the city\n",
    "    \n",
    "monthly_city_temp_df\n",
    "    city_code: city port code\n",
    "    year: year\n",
    "    month: month \n",
    "    avg_temperature: average temperature in city for given month\n",
    "    \n",
    "time_df\n",
    "    date: date\n",
    "    dayofweek: day of the week\n",
    "    weekofyear: week of year\n",
    "    month: month\n",
    "```\n",
    "\n",
    "##### Fact Table\n",
    "```\n",
    "immigration_df\n",
    "    id: id\n",
    "    state_code: state code of arrival city\n",
    "    city_code: city port code of arrival city\n",
    "    date: date of arrival\n",
    "    count: count of immigrant's entries into the US\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "Spark is chosen for this project as it is known for processing large amount of data fast (with in-memory compute), scale easily with additional worker nodes, with ability to digest different data formats (e.g. SAS, Parquet, CSV), and integrate nicely with cloud storage like S3 and warehouse like Redshift.\n",
    "\n",
    "The data update cycle is typically chosen on two criteria. One is the reporting cycle, the other is the availabilty of new data to be fed into the system. For example, if new batch of average temperature can be made available at monthly interval, we might settle for monthly data refreshing cycle.\n",
    "\n",
    "There are also considerations in terms of scaling existing solution.\n",
    "* **If the data was increased by 100x:**\n",
    "We can consider spinning up larger instances of EC2s hosting Spark and/or additional Spark work nodes. With added capacity arising from either vertical scaling or horizontal scaling, we should be able to accelerate processing time.\n",
    "\n",
    "* **If the data populates a dashboard that must be updated on a daily basis by 7am every day:**\n",
    "We can consider using Airflow to schedule and automate the data pipeline jobs. Built-in retry and monitoring mechanism can enable us to meet user requirement.\n",
    "\n",
    "* **If the database needed to be accessed by 100+ people:**\n",
    "We can consider hosting our solution in production scale data warehouse in the cloud, with larger capacity to serve more users, and workload management to ensure equitable usage of resources across users."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
